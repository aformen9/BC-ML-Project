Notebook principal.

codigo final: 

# --- 3. Regresión Logística ---
logreg = LogisticRegression(max_iter=10000, random_state=42)  # Instanciamos el modelo
logreg.fit(X_train_scaled, y_train)                          # Entrenamos con datos escalados
y_pred_log = logreg.predict(X_test_scaled)                   # Predicción de etiquetas
y_prob_log = logreg.predict_proba(X_test_scaled)[:,1]        # Probabilidades de clase positiva

# Métricas
acc_log = accuracy_score(y_test, y_pred_log)
prec_log = precision_score(y_test, y_pred_log)
rec_log = recall_score(y_test, y_pred_log)
roc_log = roc_auc_score(y_test, y_prob_log)

# Mostrar resultados
print("=== Regresión Logística ===")
print(f"Accuracy:  {acc_log:.4f}")
print(f"Precision: {prec_log:.4f}")
print(f"Recall:    {rec_log:.4f}")
print(f"ROC AUC:   {roc_log:.4f}")

# Matriz de confusión
cm_log = confusion_matrix(y_test, y_pred_log)
plt.figure(figsize=(5,4))
sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# --- 4. Random Forest ---
rf = RandomForestClassifier(n_estimators=100, random_state=42)  # Instanciamos Random Forest
rf.fit(X_train_scaled, y_train)                                 # Entrenamos
y_pred_rf = rf.predict(X_test_scaled)                           # Predicción
y_prob_rf = rf.predict_proba(X_test_scaled)[:,1]                # Probabilidades

# Métricas
acc_rf = accuracy_score(y_test, y_pred_rf)
prec_rf = precision_score(y_test, y_pred_rf)
rec_rf = recall_score(y_test, y_pred_rf)
roc_rf = roc_auc_score(y_test, y_prob_rf)

print("\n=== Random Forest ===")
print(f"Accuracy:  {acc_rf:.4f}")
print(f"Precision: {prec_rf:.4f}")
print(f"Recall:    {rec_rf:.4f}")
print(f"ROC AUC:   {roc_rf:.4f}")

cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(5,4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens')
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# --- 5. Decision Tree ---
dt = DecisionTreeClassifier(random_state=42)  # Instanciamos Decision Tree
dt.fit(X_train_scaled, y_train)               # Entrenamos
y_pred_dt = dt.predict(X_test_scaled)         # Predicción
y_prob_dt = dt.predict_proba(X_test_scaled)[:,1]  # Probabilidades de la clase positiva

# Métricas
acc_dt = accuracy_score(y_test, y_pred_dt)
prec_dt = precision_score(y_test, y_pred_dt)
rec_dt = recall_score(y_test, y_pred_dt)
roc_dt = roc_auc_score(y_test, y_prob_dt)

print("\n=== Decision Tree ===")
print(f"Accuracy:  {acc_dt:.4f}")
print(f"Precision: {prec_dt:.4f}")
print(f"Recall:    {rec_dt:.4f}")
print(f"ROC AUC:   {roc_dt:.4f}")

cm_dt = confusion_matrix(y_test, y_pred_dt)
plt.figure(figsize=(5,4))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Oranges')
plt.title('Confusion Matrix - Decision Tree')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# --- 6. Comparación de modelos ---
df_results = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest', 'Decision Tree'],
    'Accuracy': [acc_log, acc_rf, acc_dt],
    'Precision': [prec_log, prec_rf, prec_dt],
    'Recall': [rec_log, rec_rf, rec_dt],
    'ROC AUC': [roc_log, roc_rf, roc_dt]
})
print("\n=== Comparación de Modelos ===")
print(df_results)

# --- 7. Importancia de features (Random Forest) ---
importances = rf.feature_importances_            # Extraer importancias
indices = np.argsort(importances)[::-1]          # Índices ordenados descendentemente
top_n = 10                                       # Mostrar top 10

plt.figure(figsize=(8,5))
sns.barplot(
    x=importances[indices][:top_n],
    y=X_reduced.columns[indices][:top_n],
    palette='viridis'
)
plt.title('Top 10 Feature Importances - Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()
